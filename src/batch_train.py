import os
import logging
import unicodedata
import pandas as pd
from sqlalchemy import text
from collections import Counter
from database import get_db_engine

# --- è¨­å®š Log ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("training.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def normalize_text(text_str):
    """
    è³‡æ–™æ¸…æ´—é‚è¼¯ï¼š
    1. å»é™¤å‰å¾Œç©ºç™½ (trim)
    2. å…¨å½¢è½‰åŠå½¢ (NFKC normalization)
    """
    if not text_str:
        return ""
    # NFKC å¯ä»¥å°‡å…¨å½¢è‹±æ–‡/æ•¸å­—/ç©ºç™½è½‰ç‚ºåŠå½¢
    return unicodedata.normalize('NFKC', str(text_str)).strip()

def train_model():
    engine = get_db_engine()
    if not engine:
        return

    logging.info("ğŸš€ é–‹å§‹åŸ·è¡Œæ‰¹æ¬¡è¨“ç·´ (Batch Training)...")

    try:
        # 1. æ’ˆå–è³‡æ–™ (åªæ’ˆå–å¿…è¦æ¬„ä½ä»¥ç¯€çœè¨˜æ†¶é«”)
        # å¿…é ˆç¢ºä¿ mawb_no èˆ‡ hawb_no éƒ½ä¸ç‚ºç©º
        logging.info("æ­£åœ¨è®€å–æ­·å²è³‡æ–™ (Table A & Table B)...")
        
        sql_a = """
        SELECT mawb_no, hawb_no, item_no, description_original 
        FROM table_a_raw 
        WHERE mawb_no IS NOT NULL AND hawb_no IS NOT NULL 
          AND description_original IS NOT NULL
        """
        
        sql_b = """
        SELECT mawb_no, hawb_no, item_sequence, description_official, ccc_code
        FROM table_b_history
        WHERE mawb_no IS NOT NULL AND hawb_no IS NOT NULL
        """

        df_a = pd.read_sql(sql_a, engine)
        df_b = pd.read_sql(sql_b, engine)

        # è³‡æ–™å‰è™•ç† (æ¸…æ´— Key å€¼ä»¥ä¾¿ Join)
        # ç§»é™¤ mawb/hawb çš„ç©ºç™½èˆ‡ç¬¦è™Ÿï¼Œç¢ºä¿å°æ‡‰ç‡
        for df in [df_a, df_b]:
            df['mawb_clean'] = df['mawb_no'].astype(str).str.replace(r'[\s/-]', '', regex=True).str.upper()
            df['hawb_clean'] = df['hawb_no'].astype(str).str.replace(r'[\s/-]', '', regex=True).str.upper()
            df['link_key'] = df['mawb_clean'] + "_" + df['hawb_clean']

        # 2. æª¢æ ¸é …æ¬¡æ•¸é‡ (Consolidation Logic)
        # çµ±è¨ˆæ¯å€‹åˆ†æå–®(Key)æœ‰å¤šå°‘å€‹é …æ¬¡
        count_a = df_a.groupby('link_key').size()
        count_b = df_b.groupby('link_key').size()

        # æ‰¾å‡ºé …æ¬¡æ•¸é‡å®Œå…¨ä¸€è‡´çš„ Keys (äº¤é›†)
        valid_keys = count_a.index.intersection(count_b.index)
        
        # é€²ä¸€æ­¥éæ¿¾ï¼šæ•¸é‡å¿…é ˆç›¸ç­‰ (Count A == Count B)
        # é€™è£¡åˆ©ç”¨ Pandas çš„å‘é‡é‹ç®—å¿«é€Ÿæ¯”å°
        matched_counts_mask = (count_a[valid_keys] == count_b[valid_keys])
        final_valid_keys = valid_keys[matched_counts_mask]

        logging.info(f"ç¸½åˆ†æå–®æ•¸: A={len(count_a)}, B={len(count_b)}")
        logging.info(f"ç¬¦åˆã€Œé …æ¬¡æ•¸é‡ä¸€è‡´(1å°1)ã€çš„æœ‰æ•ˆè¨“ç·´å–®æ•¸: {len(final_valid_keys)}")

        if len(final_valid_keys) == 0:
            logging.warning("æ²’æœ‰ç¬¦åˆè¨“ç·´æ¢ä»¶çš„è³‡æ–™ã€‚è«‹ç¢ºèª Table A èˆ‡ B æ˜¯å¦æœ‰æˆå°çš„ä¸»/åˆ†æå–®è™Ÿã€‚")
            return

        # 3. å»ºç«‹è¨“ç·´é›† (Linking)
        # åªä¿ç•™æœ‰æ•ˆçš„è³‡æ–™
        df_a_clean = df_a[df_a['link_key'].isin(final_valid_keys)].copy()
        df_b_clean = df_b[df_b['link_key'].isin(final_valid_keys)].copy()

        # æ’åºï¼šç¢ºä¿æŒ‰ç…§ item_no / item_sequence é †åºæ’åˆ—ï¼Œä»¥ä¾¿ä¾åºé…å°
        df_a_clean.sort_values(by=['link_key', 'item_no'], inplace=True)
        df_b_clean.sort_values(by=['link_key', 'item_sequence'], inplace=True)

        # é‡ç½®ç´¢å¼•ï¼Œåˆ©ç”¨ä½ç½® (Reset Index) ä¾†å¼·åˆ¶å°é½Š
        # å› ç‚ºå·²çŸ¥æ¯å€‹ Key è£¡çš„æ•¸é‡ä¸€æ¨£ï¼Œæ’åºå¾Œç¬¬ 1 ç­† A å¿…å®šå°æ‡‰ç¬¬ 1 ç­† B
        # é€™è£¡æˆ‘å€‘ä½¿ç”¨ä¸€å€‹æŠ€å·§ï¼šç›´æ¥æŠŠå…©å€‹ DF çš„å…§å®¹åˆä½µ
        
        # æå–éœ€è¦çš„æ¬„ä½åˆ—è¡¨
        train_source = df_a_clean['description_original'].apply(normalize_text).tolist()
        train_target_desc = df_b_clean['description_official'].tolist()
        train_target_ccc = df_b_clean['ccc_code'].tolist()

        # 4. å¤šæ•¸æ±ºæŠ•ç¥¨ (Majority Vote)
        logging.info("æ­£åœ¨é€²è¡ŒçŸ¥è­˜èƒå–èˆ‡å¤šæ•¸æ±ºæŠ•ç¥¨...")
        
        # çµæ§‹: { åŸå§‹å“å: Counter( (æ¨™æº–å“å, ç¨…è™Ÿ) ) }
        knowledge_map = {}

        for src, tgt_desc, tgt_ccc in zip(train_source, train_target_desc, train_target_ccc):
            if not src: continue
            
            if src not in knowledge_map:
                knowledge_map[src] = Counter()
            
            # æŠ•ç¥¨ï¼šé€™çµ„å°æ‡‰å‡ºç¾ä¸€æ¬¡ï¼Œå°±åŠ ä¸€ç¥¨
            knowledge_map[src][(tgt_desc, tgt_ccc)] += 1

        # 5. ç”¢ç”Ÿæœ€çµ‚çŸ¥è­˜åº« (Winner Takes All)
        final_records = []
        for src_desc, counter in knowledge_map.items():
            # å–å¾—ç¥¨æ•¸æœ€é«˜çš„çµ„åˆ (most_common(1) å›å‚³ [((desc, ccc), count)])
            winner, count = counter.most_common(1)[0]
            official_desc, ccc = winner
            
            final_records.append({
                'original_description': src_desc,
                'official_description': official_desc,
                'ccc_code': ccc,
                'frequency': count
            })

        # 6. å¯«å…¥è³‡æ–™åº« (Update Database)
        logging.info(f"å­¸ç¿’å®Œæˆï¼Œå…±æå– {len(final_records)} æ¢æ¨™æº–çŸ¥è­˜ã€‚æº–å‚™å¯«å…¥...")
        
        if final_records:
            df_knowledge = pd.DataFrame(final_records)
            
            # ä½¿ç”¨ temp table ç­–ç•¥é€²è¡Œ Upsert (æ›´æ–°æˆ–æ’å…¥)
            # å› ç‚º Pandas to_sql é è¨­åªæœ‰ fail, replace, append
            # æˆ‘å€‘å¸Œæœ›ä¿ç•™èˆŠè³‡æ–™ä½†æ›´æ–°é »ç‡ -> å…¶å¯¦æœ€ç°¡å–®æ˜¯ Truncate é‡å»º (å› ç‚ºæ˜¯ Batch Train)
            # æ ¹æ“šæ‚¨çš„æŒ‡ç¤º "ä¸€æ¬¡æ€§è·‘å®Œ... ç³»çµ±ç¬¬ä¸€å¤©å°±å¾ˆè°æ˜"ï¼Œæ¸…ç©ºé‡å»ºæ˜¯æœ€ä¹¾æ·¨çš„
            
            with engine.begin() as conn:
                conn.execute(text("TRUNCATE TABLE standard_knowledge_base"))
                df_knowledge.to_sql('standard_knowledge_base', conn, if_exists='append', index=False)
            
            logging.info("âœ… æ¨™æº–çŸ¥è­˜åº«å·²æ›´æ–°å®Œç•¢ï¼")

    except Exception as e:
        logging.error(f"è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

if __name__ == "__main__":
    train_model()