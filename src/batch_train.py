import os
import re
import logging
import unicodedata
import pandas as pd
from datetime import datetime
from sqlalchemy import text
from collections import Counter
from database import get_db_engine

# --- è¨­å®š Log ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("training.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def normalize_text(text_str):
    """
    è³‡æ–™æ¸…æ´—é‚è¼¯ï¼š
    1. å…¨å½¢è½‰åŠå½¢ (NFKC)
    2. å¼·åˆ¶è½‰å¤§å¯« (.upper)
    3. [æ–°å¢] é‡å° '/' ç¬¦è™Ÿè™•ç†ï¼šåªä¿ç•™ '/' ä¹‹å¾Œçš„æ–‡å­— (å–æœ€å¾Œä¸€æ®µ)
    4. ç§»é™¤æ¨™é»ç¬¦è™Ÿèˆ‡ç‰¹æ®Šå­—å…ƒ (å°‡å…¶æ›¿æ›ç‚ºç©ºç™½)
    5. ç¸®æ¸›å¤šé¤˜ç©ºç™½
    """
    if not text_str:
        return ""
    
    # 1. NFKC æ¨™æº–åŒ– (å…¨å½¢è½‰åŠå½¢)
    text_val = unicodedata.normalize('NFKC', str(text_str))
    
    # 2. å¼·åˆ¶è½‰å¤§å¯«
    text_val = text_val.upper()

    # 3. [æ–°å¢éœ€æ±‚] é‡å° '/' è™•ç†ï¼šæ¸…é™¤ '/' ä¹‹å‰çš„æ–‡å­—ï¼Œåªä¿ç•™ä¹‹å¾Œçš„
    # ä¾‹å¦‚ "è‹±æ–‡/ä¸­æ–‡" -> "ä¸­æ–‡", "A/B/C" -> "C" (å–æœ€å¾Œä¸€æ®µæœ€ç²¾ç¢º)
    if '/' in text_val:
        text_val = text_val.split('/')[-1]
    
    # 4. ä½¿ç”¨ Regex æ›¿æ›æ¨™é»ç¬¦è™Ÿç‚ºç©ºç™½
    # [^\w\s] è¡¨ç¤ºåŒ¹é… "é(æ–‡å­—ã€æ•¸å­—ã€åº•ç·šã€ç©ºç™½)" çš„æ‰€æœ‰å­—å…ƒ
    # é€™æœƒæŠŠ -, (, ), @ ç­‰ç¬¦è™Ÿéƒ½è®Šæˆç©ºç™½ï¼Œé¿å…é»åœ¨ä¸€èµ·
    text_val = re.sub(r'[^\w\s]', ' ', text_val)
    
    # 5. ç¸®æ¸›å¤šé¤˜ç©ºç™½ (å°‡é€£çºŒç©ºç™½è®Šç‚ºä¸€å€‹) ä¸¦ å»é™¤å‰å¾Œç©ºç™½
    text_val = re.sub(r'\s+', ' ', text_val).strip()
    
    return text_val

def train_model():
    engine = get_db_engine()
    if not engine:
        return

    logging.info("ğŸš€ é–‹å§‹åŸ·è¡Œæ‰¹æ¬¡è¨“ç·´ (Batch Training)...")

    try:
        # 1. æ’ˆå–è³‡æ–™ (åªæ’ˆå–å¿…è¦æ¬„ä½ä»¥ç¯€çœè¨˜æ†¶é«”)
        # å¿…é ˆç¢ºä¿ mawb_no èˆ‡ hawb_no éƒ½ä¸ç‚ºç©º
        logging.info("æ­£åœ¨è®€å–æ­·å²è³‡æ–™ (Table A & Table B)...")
        
        sql_a = """
        SELECT mawb_no, hawb_no, item_no, description_original 
        FROM table_a_raw 
        WHERE mawb_no IS NOT NULL AND hawb_no IS NOT NULL 
          AND description_original IS NOT NULL
        """
        
        sql_b = """
        SELECT mawb_no, hawb_no, item_sequence, description_official, ccc_code
        FROM table_b_history
        WHERE mawb_no IS NOT NULL AND hawb_no IS NOT NULL
        """

        df_a = pd.read_sql(sql_a, engine)
        df_b = pd.read_sql(sql_b, engine)

        # è³‡æ–™å‰è™•ç† (æ¸…æ´— Key å€¼ä»¥ä¾¿ Join)
        # ç§»é™¤ mawb/hawb çš„ç©ºç™½èˆ‡ç¬¦è™Ÿï¼Œç¢ºä¿å°æ‡‰ç‡
        for df in [df_a, df_b]:
            df['mawb_clean'] = df['mawb_no'].astype(str).str.replace(r'[\s/-]', '', regex=True).str.upper()
            df['hawb_clean'] = df['hawb_no'].astype(str).str.replace(r'[\s/-]', '', regex=True).str.upper()
            df['link_key'] = df['mawb_clean'] + "_" + df['hawb_clean']

        # 2. æª¢æ ¸é …æ¬¡æ•¸é‡ (Consolidation Logic)
        # çµ±è¨ˆæ¯å€‹åˆ†æå–®(Key)æœ‰å¤šå°‘å€‹é …æ¬¡
        count_a = df_a.groupby('link_key').size()
        count_b = df_b.groupby('link_key').size()

        # æ‰¾å‡ºé …æ¬¡æ•¸é‡å®Œå…¨ä¸€è‡´çš„ Keys (äº¤é›†)
        valid_keys = count_a.index.intersection(count_b.index)
        
        # é€²ä¸€æ­¥éæ¿¾ï¼šæ•¸é‡å¿…é ˆç›¸ç­‰ (Count A == Count B)
        matched_counts_mask = (count_a[valid_keys] == count_b[valid_keys])
        final_valid_keys = valid_keys[matched_counts_mask]

        logging.info(f"ç¸½åˆ†æå–®æ•¸: A={len(count_a)}, B={len(count_b)}")
        logging.info(f"ç¬¦åˆã€Œé …æ¬¡æ•¸é‡ä¸€è‡´(1å°1)ã€çš„æœ‰æ•ˆè¨“ç·´å–®æ•¸: {len(final_valid_keys)}")

        if len(final_valid_keys) == 0:
            logging.warning("æ²’æœ‰ç¬¦åˆè¨“ç·´æ¢ä»¶çš„è³‡æ–™ã€‚è«‹ç¢ºèª Table A èˆ‡ B æ˜¯å¦æœ‰æˆå°çš„ä¸»/åˆ†æå–®è™Ÿã€‚")
            return

        # 3. å»ºç«‹è¨“ç·´é›† (Linking)
        df_a_clean = df_a[df_a['link_key'].isin(final_valid_keys)].copy()
        df_b_clean = df_b[df_b['link_key'].isin(final_valid_keys)].copy()

        # æ’åº
        df_a_clean.sort_values(by=['link_key', 'item_no'], inplace=True)
        df_b_clean.sort_values(by=['link_key', 'item_sequence'], inplace=True)

        # æå–æ¬„ä½ä¸¦é€²è¡Œæ¸…æ´— (Normalize)
        train_source = df_a_clean['description_original'].apply(normalize_text).tolist()
        train_target_desc = df_b_clean['description_official'].tolist()
        train_target_ccc = df_b_clean['ccc_code'].tolist()

        # 4. å¤šæ•¸æ±ºæŠ•ç¥¨ (Majority Vote)
        logging.info("æ­£åœ¨é€²è¡ŒçŸ¥è­˜èƒå–èˆ‡å¤šæ•¸æ±ºæŠ•ç¥¨...")
        
        knowledge_map = {}

        for src, tgt_desc, tgt_ccc in zip(train_source, train_target_desc, train_target_ccc):
            if not src: continue # ç•¥éç©ºå­—ä¸²
            
            if src not in knowledge_map:
                knowledge_map[src] = Counter()
            
            # æŠ•ç¥¨
            knowledge_map[src][(tgt_desc, tgt_ccc)] += 1

        # 5. ç”¢ç”Ÿæœ€çµ‚çŸ¥è­˜åº« (Winner Takes All)
        final_records = []
        for src_desc, counter in knowledge_map.items():
            winner, count = counter.most_common(1)[0]
            official_desc, ccc = winner
            
            final_records.append({
                'original_description': src_desc,
                'official_description': official_desc,
                'ccc_code': ccc,
                'frequency': count
            })

        # 6. è³‡æ–™åº«æ“ä½œ (å‚™ä»½ -> æ¸…ç©º -> å¯«å…¥)
        logging.info(f"å­¸ç¿’å®Œæˆï¼Œå…±æå– {len(final_records)} æ¢æ¨™æº–çŸ¥è­˜ã€‚")
        
        if final_records:
            df_knowledge = pd.DataFrame(final_records)
            
            with engine.begin() as conn:
                # [æ–°å¢] è‡ªå‹•å‚™ä»½æ©Ÿåˆ¶
                # æª¢æŸ¥ç›®å‰æ¨™æº–åº«æ˜¯å¦æœ‰è³‡æ–™
                check_sql = text("SELECT COUNT(*) FROM standard_knowledge_base")
                row_count = conn.execute(check_sql).scalar()
                
                if row_count > 0:
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    backup_table = f"standard_knowledge_base_backup_{timestamp}"
                    logging.info(f"åµæ¸¬åˆ°èˆŠè³‡æ–™ ({row_count} ç­†)ï¼Œæ­£åœ¨å‚™ä»½è‡³ {backup_table} ...")
                    
                    # åŸ·è¡Œå‚™ä»½ (Create Table As Select)
                    backup_sql = text(f"CREATE TABLE {backup_table} AS SELECT * FROM standard_knowledge_base")
                    conn.execute(backup_sql)
                    logging.info("å‚™ä»½å®Œæˆã€‚")

                # æ¸…ç©ºèˆŠè³‡æ–™
                logging.info("æ­£åœ¨æ¸…ç©ºæ¨™æº–çŸ¥è­˜åº« (TRUNCATE)...")
                conn.execute(text("TRUNCATE TABLE standard_knowledge_base"))
                
                # å¯«å…¥æ–°è³‡æ–™
                logging.info("æ­£åœ¨å¯«å…¥æ–°è¨“ç·´è³‡æ–™...")
                df_knowledge.to_sql('standard_knowledge_base', conn, if_exists='append', index=False)
            
            logging.info("âœ… æ¨™æº–çŸ¥è­˜åº«å·²æ›´æ–°å®Œç•¢ï¼")

    except Exception as e:
        logging.error(f"è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

if __name__ == "__main__":
    train_model()